{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDyUmhfopb4S"
      },
      "source": [
        "# Evaluating discovered update rules on Sokoban\n",
        "\n",
        "This colab demonstrates how to instantiate the `Disco103` update rule and use it for training an RL agent on the `Sokoban` environment.\n",
        "\n",
        "The repository also contains `ActorCritic` and `PolicyGradient` update rules and a CPU version of `Catch`; feel free to explore and repurpose this code for your needs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Q_QQRMPQaLip"
      },
      "outputs": [],
      "source": [
        "# @title Install the package.\n",
        "\n",
        "# !pip install git+https://github.com/google-deepmind/disco_rl.git\n",
        "\n",
        "import collections\n",
        "\n",
        "import chex\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import rlax\n",
        "import seaborn as sns\n",
        "import tqdm\n",
        "\n",
        "# Types & utils\n",
        "from disco_rl import types\n",
        "from disco_rl import utils\n",
        "\n",
        "# Environments\n",
        "from disco_rl.environments import base as base_env\n",
        "from disco_rl.environments import jittable_envs\n",
        "\n",
        "# Learning\n",
        "from disco_rl import agent as agent_lib\n",
        "\n",
        "axis_name = 'i'  # for parallelisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'sokoban'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sokoban_path \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m sys.path:\n\u001b[32m      9\u001b[39m   sys.path.insert(\u001b[32m0\u001b[39m, sokoban_path)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msokoban\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mathisfun_sokoban \u001b[38;5;28;01mas\u001b[39;00m mfs\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdm_env\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdm_env\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m specs\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'sokoban'"
          ]
        }
      ],
      "source": [
        "# @title Gym to Disco environment adapter for Sokoban\n",
        "\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Add sokoban directory to path (works from colabs directory)\n",
        "sokoban_path = os.path.join(os.path.dirname(os.getcwd()), 'sokoban')\n",
        "if sokoban_path not in sys.path:\n",
        "  sys.path.insert(0, sokoban_path)\n",
        "\n",
        "from sokoban import mathisfun_sokoban as mfs\n",
        "import dm_env\n",
        "from dm_env import specs\n",
        "\n",
        "class GymToDiscoEnv(base_env.Environment):\n",
        "  \"\"\"Adapter that wraps Gymnasium Sokoban environment for Disco RL.\"\"\"\n",
        "  \n",
        "  def __init__(self, batch_size: int = 1, gamma: float = 0.99, seed: int = 0):\n",
        "    self.batch_size = batch_size\n",
        "    self.gamma = gamma\n",
        "    \n",
        "    # Create the gym environment\n",
        "    self._gym_env = mfs.MathIsFunSokoban()\n",
        "    # Reset to initialize\n",
        "    self._gym_env.reset(seed=seed)\n",
        "    \n",
        "  def single_observation_spec(self) -> types.Specs:\n",
        "    \"\"\"Returns the observation spec.\"\"\"\n",
        "    obs_space = self._gym_env.observation_space\n",
        "    # Convert to float32 and normalize to [0, 1]\n",
        "    return {\n",
        "      'observation': specs.Array(\n",
        "        shape=obs_space.shape,\n",
        "        dtype=jnp.float32,\n",
        "        name='observation'\n",
        "      )\n",
        "    }\n",
        "  \n",
        "  def single_action_spec(self) -> types.ActionSpec:\n",
        "    \"\"\"Returns the action spec.\"\"\"\n",
        "    n = self._gym_env.action_space.n\n",
        "    return specs.BoundedArray(\n",
        "      shape=(),\n",
        "      dtype=jnp.int32,\n",
        "      minimum=0,\n",
        "      maximum=n - 1,\n",
        "      name='action'\n",
        "    )\n",
        "  \n",
        "  def reset(\n",
        "      self, rng_key: chex.PRNGKey\n",
        "  ) -> tuple[dict, types.EnvironmentTimestep]:\n",
        "    \"\"\"Resets the environment.\"\"\"\n",
        "    # Use seed from rng if available, otherwise use default\n",
        "    obs, info = self._gym_env.reset()\n",
        "    \n",
        "    # Normalize observation to [0, 1] and convert to float32\n",
        "    obs_normalized = jnp.asarray(obs, dtype=jnp.float32) / 255.0\n",
        "    \n",
        "    timestep = types.EnvironmentTimestep(\n",
        "      observation={'observation': obs_normalized},\n",
        "      step_type=jnp.array(dm_env.StepType.FIRST, dtype=jnp.int32),\n",
        "      reward=jnp.array(0.0, dtype=jnp.float32)\n",
        "    )\n",
        "    \n",
        "    return {}, timestep\n",
        "  \n",
        "  def step(\n",
        "      self, state: dict, actions: chex.ArrayTree\n",
        "  ) -> tuple[dict, types.EnvironmentTimestep]:\n",
        "    \"\"\"Steps the environment.\"\"\"\n",
        "    # Extract action (handle batched case)\n",
        "    actions_array = jnp.asarray(actions)\n",
        "    if actions_array.shape == ():\n",
        "      action = int(actions_array)\n",
        "    elif len(actions_array.shape) == 1 and actions_array.shape[0] == 1:\n",
        "      action = int(actions_array[0])\n",
        "    else:\n",
        "      action = int(actions_array.flatten()[0])\n",
        "    \n",
        "    obs, reward, terminated, truncated, info = self._gym_env.step(action)\n",
        "    done = terminated or truncated\n",
        "    \n",
        "    # Normalize observation to [0, 1] and convert to float32\n",
        "    obs_normalized = jnp.asarray(obs, dtype=jnp.float32) / 255.0\n",
        "    \n",
        "    # Determine step type\n",
        "    if done:\n",
        "      step_type = dm_env.StepType.LAST\n",
        "    else:\n",
        "      step_type = dm_env.StepType.MID\n",
        "    \n",
        "    # Auto-reset if done\n",
        "    if done:\n",
        "      obs, _ = self._gym_env.reset()\n",
        "      obs_normalized = jnp.asarray(obs, dtype=jnp.float32) / 255.0\n",
        "    \n",
        "    timestep = types.EnvironmentTimestep(\n",
        "      observation={'observation': obs_normalized},\n",
        "      step_type=jnp.array(step_type, dtype=jnp.int32),\n",
        "      reward=jnp.array(reward, dtype=jnp.float32)\n",
        "    )\n",
        "    \n",
        "    return state, timestep\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hinBYQVJqlm0"
      },
      "outputs": [],
      "source": [
        "# @title Download and unpack `Disco103` weights.\n",
        "\n",
        "def unflatten_params(flat_params: chex.ArrayTree) -> chex.ArrayTree:\n",
        "  params = {}\n",
        "  for key_wb in flat_params:\n",
        "    key = '/'.join(key_wb.split('/')[:-1])\n",
        "    params[key] = {\n",
        "        'b': flat_params[f'{key}/b'],\n",
        "        'w': flat_params[f'{key}/w'],\n",
        "    }\n",
        "  return params\n",
        "\n",
        "\n",
        "disco_103_fname = 'disco_103.npz'\n",
        "disco_103_url = f\"https://raw.githubusercontent.com/google-deepmind/disco_rl/main/disco_rl/update_rules/weights/{disco_103_fname}\"\n",
        "# !wget $disco_103_url\n",
        "\n",
        "with open(f'/home/skr/Downloads/disco_rl/colabs/disco_103.npz', 'rb') as file:\n",
        "  disco_103_params = unflatten_params(np.load(file))\n",
        "\n",
        "print(f'Loaded {len(disco_103_params) * 2} parameter tensors for Disco103.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QhHP6mbdE9Rx"
      },
      "outputs": [],
      "source": [
        "# @title Instantiate a simple MLP agent.\n",
        "\n",
        "\n",
        "def get_env(batch_size: int) -> base_env.Environment:\n",
        "  return GymToDiscoEnv(batch_size=batch_size, gamma=0.99, seed=0)\n",
        "\n",
        "\n",
        "# Create a dummy environment.\n",
        "env = get_env(batch_size=1)\n",
        "\n",
        "# Create settings for an agent.\n",
        "agent_settings = agent_lib.get_settings_disco()\n",
        "agent_settings.net_settings.name = 'mlp'\n",
        "agent_settings.net_settings.net_args = dict(\n",
        "    dense=(512, 512),\n",
        "    model_arch_name='lstm',\n",
        "    head_w_init_std=1e-2,\n",
        "    model_kwargs=dict(\n",
        "        head_mlp_hiddens=(128,),\n",
        "        lstm_size=128,\n",
        "    ),\n",
        ")\n",
        "agent_settings.learning_rate = 1e-2\n",
        "\n",
        "# Create the agent.\n",
        "agent = agent_lib.Agent(\n",
        "    agent_settings=agent_settings,\n",
        "    single_observation_spec=env.single_observation_spec(),\n",
        "    single_action_spec=env.single_action_spec(),\n",
        "    batch_axis_name=axis_name,\n",
        ")\n",
        "\n",
        "# Ensure that the agent's update rule's parameters have the same specs.\n",
        "random_update_rule_params, _ = agent.update_rule.init_params(\n",
        "    jax.random.PRNGKey(0)\n",
        ")\n",
        "if agent_settings.update_rule_name == 'disco':\n",
        "  chex.assert_trees_all_equal_shapes_and_dtypes(\n",
        "      random_update_rule_params, disco_103_params\n",
        "  )\n",
        "  print('Update rule parameters have the same specs.')\n",
        "else:\n",
        "  print('Not using a discovered rule, skipping check.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZnd20Cph9Sl"
      },
      "outputs": [],
      "source": [
        "# @title Helper functions for interacting with environments.\n",
        "def unroll_cpu_actor(\n",
        "    params,\n",
        "    actor_state,\n",
        "    ts,\n",
        "    env_state,\n",
        "    rng,\n",
        "    env,\n",
        "    rollout_len,\n",
        "    actor_step_fn,\n",
        "    devices,\n",
        "):\n",
        "  \"\"\"Unrolls the policy for a CPU environments.\"\"\"\n",
        "  del devices  # Not needed for single device\n",
        "  actor_timesteps = []\n",
        "  for _ in range(rollout_len):\n",
        "    rng, step_rng = jax.random.split(rng)\n",
        "\n",
        "    actor_timestep, actor_state = actor_step_fn(\n",
        "        params, step_rng, ts, actor_state\n",
        "    )\n",
        "    # Extract action - for batch_size=1, actions is a scalar or shape (1,)\n",
        "    actions = actor_timestep.actions\n",
        "    if hasattr(actions, 'shape'):\n",
        "      if actions.shape == ():\n",
        "        action = int(actions)\n",
        "      elif len(actions.shape) == 1 and actions.shape[0] == 1:\n",
        "        action = int(actions[0])\n",
        "      else:\n",
        "        action = int(actions.flatten()[0])\n",
        "    else:\n",
        "      action = int(actions)\n",
        "    env_state, ts = env.step(env_state, action)\n",
        "\n",
        "    actor_timesteps.append(actor_timestep)\n",
        "\n",
        "  actor_rollout = types.ActorRollout.from_timestep(\n",
        "      utils.tree_stack(actor_timesteps, axis=0)\n",
        "  )\n",
        "  return actor_rollout, actor_state, ts, env_state\n",
        "\n",
        "\n",
        "def unroll_jittable_actor(\n",
        "    params,\n",
        "    actor_state,\n",
        "    ts,\n",
        "    env_state,\n",
        "    rng,\n",
        "    env,\n",
        "    rollout_len,\n",
        "    actor_step_fn,\n",
        "    devices,\n",
        "):\n",
        "  \"\"\"Unrolls the policy for a jittable environment.\"\"\"\n",
        "  del actor_step_fn, devices\n",
        "\n",
        "  def _single_step(carry, step_rng):\n",
        "    env_state, ts, actor_state = carry\n",
        "    actor_timestep, actor_state = agent.actor_step(\n",
        "        params, step_rng, ts, actor_state\n",
        "    )\n",
        "    env_state, ts = env.step(env_state, actor_timestep.actions)\n",
        "    return (env_state, ts, actor_state), actor_timestep\n",
        "\n",
        "  (env_state, ts, actor_state), actor_rollout = jax.lax.scan(\n",
        "      _single_step,\n",
        "      (env_state, ts, actor_state),\n",
        "      jax.random.split(rng, rollout_len),\n",
        "  )\n",
        "\n",
        "  actor_rollout = types.ActorRollout.from_timestep(actor_rollout)\n",
        "  return actor_rollout, actor_state, ts, env_state\n",
        "\n",
        "\n",
        "def accumulate_rewards(acc_rewards, x):\n",
        "  rewards, discounts = x\n",
        "\n",
        "  def _step_fn(acc_rewards, x):\n",
        "    rewards, discounts = x\n",
        "    acc_rewards += rewards\n",
        "    return acc_rewards * discounts, acc_rewards\n",
        "\n",
        "  return jax.lax.scan(_step_fn, acc_rewards, (rewards, discounts))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lJ4HfawLSMWH"
      },
      "outputs": [],
      "source": [
        "# @title A simple Replay buffer.\n",
        "\n",
        "\n",
        "class SimpleReplayBuffer:\n",
        "  \"\"\"A simple FIFO replay buffer for JAX arrays.\"\"\"\n",
        "\n",
        "  def __init__(self, capacity: int, seed: int):\n",
        "    \"\"\"Initializes the buffer.\"\"\"\n",
        "    self.buffer = collections.deque(maxlen=capacity)\n",
        "    self.capacity = capacity\n",
        "    self.np_rng = np.random.default_rng(seed)\n",
        "\n",
        "  def add(self, rollout: types.ActorRollout) -> None:\n",
        "    \"\"\"Appends a batch of trajectories to the buffer.\"\"\"\n",
        "    rollout = jax.device_get(rollout)\n",
        "    # split_tree = split_tree_on_dim(rollout, 2)\n",
        "    split_tree = rlax.tree_split_leaves(rollout, axis=2)  # across batch dim\n",
        "    self.buffer.extend(split_tree)\n",
        "\n",
        "  def sample(self, batch_size: int) -> types.ActorRollout | None:\n",
        "    \"\"\"Samples a batch of trajectories from the buffer.\"\"\"\n",
        "    buffer_size = len(self.buffer)\n",
        "    if buffer_size == 0:\n",
        "      print(\"Warning: Trying to sample from an empty buffer.\")\n",
        "      return None\n",
        "\n",
        "    indices = self.np_rng.integers(buffer_size, size=batch_size)\n",
        "    batched_samples = utils.tree_stack(\n",
        "        [self.buffer[i] for i in indices], axis=2\n",
        "    )\n",
        "    return batched_samples\n",
        "\n",
        "  def __len__(self) -> int:\n",
        "    \"\"\"Returns the current number of transitions in the buffer.\"\"\"\n",
        "    return len(self.buffer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmpFAJ84HUjS"
      },
      "outputs": [],
      "source": [
        "# @title Training loop\n",
        "\n",
        "num_steps = 1000\n",
        "batch_size = 64\n",
        "rollout_len = 29\n",
        "rng_key = jax.random.PRNGKey(0)\n",
        "\n",
        "replay_ratio = 32\n",
        "buffer = SimpleReplayBuffer(capacity=1024, seed=17)\n",
        "min_buffer_size = batch_size\n",
        "\n",
        "# Use CPU mode with batch_size=1 for Sokoban\n",
        "num_envs = 1\n",
        "devices = (jax.devices()[0],)  # Single device\n",
        "env = get_env(num_envs)\n",
        "\n",
        "# Init states.\n",
        "env_state, ts = env.reset(rng_key)\n",
        "acc_rewards = jnp.zeros((num_envs,))\n",
        "learner_state = agent.initial_learner_state(rng_key)\n",
        "actor_state = agent.initial_actor_state(rng_key)\n",
        "update_rule_params = disco_103_params\n",
        "\n",
        "# Use CPU mode (no pmap) for Sokoban\n",
        "is_jittable_actor = False\n",
        "actor_step_fn = agent.actor_step\n",
        "learner_step_fn = agent.learner_step\n",
        "unroll_actor = unroll_cpu_actor\n",
        "acc_rewards_fn = accumulate_rewards\n",
        "\n",
        "# Buffers.\n",
        "all_metrics = []\n",
        "all_rewards = []\n",
        "all_discounts = []\n",
        "all_steps = []\n",
        "all_returns = []\n",
        "total_steps = 0\n",
        "\n",
        "# Run the loop.\n",
        "for step in tqdm.tqdm(range(num_steps)):\n",
        "  rng_key, rng_actor, rng_learner = jax.random.split(rng_key, 3)\n",
        "\n",
        "  # Generate new trajectories and add them to the buffer.\n",
        "  actor_rollout, actor_state, ts, env_state = unroll_actor(\n",
        "      learner_state.params,\n",
        "      actor_state,\n",
        "      ts,\n",
        "      env_state,\n",
        "      rng_actor,\n",
        "      env,\n",
        "      rollout_len,\n",
        "      actor_step_fn,\n",
        "      devices,\n",
        "  )\n",
        "  buffer.add(actor_rollout)\n",
        "\n",
        "  # Accumulate statistics.\n",
        "  total_steps += np.prod(actor_rollout.rewards.shape)\n",
        "  acc_rewards, returns = acc_rewards_fn(\n",
        "      acc_rewards,\n",
        "      (actor_rollout.rewards, actor_rollout.discounts),\n",
        "  )\n",
        "  all_steps.append(total_steps)\n",
        "  all_rewards.append(jax.device_get(actor_rollout.rewards))\n",
        "  all_discounts.append(jax.device_get(actor_rollout.discounts))\n",
        "  all_returns.append(jax.device_get(returns))\n",
        "\n",
        "  # Update agent's parameters on the samples from the buffer.\n",
        "  if len(buffer) >= min_buffer_size:\n",
        "    learner_rollout = buffer.sample(batch_size)\n",
        "    learner_state, _, metrics = learner_step_fn(\n",
        "        rng_learner,\n",
        "        learner_rollout,\n",
        "        learner_state,\n",
        "        actor_state,\n",
        "        update_rule_params,\n",
        "        False,  # is_meta_training\n",
        "    )\n",
        "    all_metrics.append(jax.device_get(metrics))\n",
        "\n",
        "# Collect all logs and statistics (no gather needed for single device)\n",
        "if all_metrics:\n",
        "  all_metrics = [m if isinstance(m, dict) else {'loss': m} for m in all_metrics]\n",
        "  # Stack metrics if needed\n",
        "  if len(all_metrics) > 0 and isinstance(all_metrics[0], dict):\n",
        "    stacked_metrics = {}\n",
        "    for key in all_metrics[0].keys():\n",
        "      stacked_metrics[key] = jnp.stack([m[key] for m in all_metrics])\n",
        "    all_metrics = stacked_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80kt3EUbIps0"
      },
      "outputs": [],
      "source": [
        "# @title Process logs\n",
        "all_returns = np.array(all_returns)\n",
        "all_discounts = np.array(all_discounts)\n",
        "all_steps = np.array(all_steps)\n",
        "total_returns = (all_returns * (1 - all_discounts)).sum(axis=(1, 2))\n",
        "total_episodes = (1 - all_discounts).sum(axis=(1, 2))\n",
        "avg_returns = total_returns / total_episodes\n",
        "\n",
        "padded_metrics = {}\n",
        "pad_width = len(all_steps) - len(all_metrics)\n",
        "for key in all_metrics[0].keys():\n",
        "  values = np.array([m[key] for m in all_metrics])\n",
        "  padded_metrics[key] = np.pad(values, (pad_width, 0), constant_values=np.nan)\n",
        "\n",
        "df = pd.DataFrame(\n",
        "    dict(\n",
        "        steps=all_steps,\n",
        "        avg_returns=avg_returns,\n",
        "        **padded_metrics,\n",
        "    )\n",
        ")\n",
        "\n",
        "df['name'] = agent_settings.update_rule_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "se4UE1W93uRt"
      },
      "outputs": [],
      "source": [
        "sns.lineplot(data=df, x='steps', y='avg_returns')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "disco",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
